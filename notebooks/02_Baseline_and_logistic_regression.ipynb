{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autores:  \n",
    "Blanco García, Gabriel: gabriel.blanco@cunef.edu  \n",
    "Ferrín Meilán, Michelle: michelle.ferrin@cunef.edu\n",
    "\n",
    "## Colegio Universitario de Estudios Financieros\n",
    "### Máster en Data Science para Finanzas\n",
    "Madrid, diciembre de 2020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelo base"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En esta sección se construye el modelo base, que servirá como referencia para comparar el resto de modelos, así como la regresión logística. También se construyen los Pipelines, herramienta fundamental para controlar y mantener en orden los pasos a seguir para el procesamiento de datos previo a los modelos. Los modelos se compararán en el último notebook, de selección de modelos, en el cual se decidirá cuál es el ganador y se calcularán diversas métricas del mismo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Manipulación\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Barajar los datos \n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "# Pipelines y preprocesado\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer \n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# Modelos: baseline y logit\n",
    "from sklearn.dummy import DummyClassifier \n",
    "from sklearn.linear_model import LogisticRegressionCV \n",
    "\n",
    "# Evaluación preliminar del modelo\n",
    "from sklearn import metrics \n",
    "\n",
    "# Guardar modelos y pipelines\n",
    "import pickle \n",
    "\n",
    "# Semilla de aleatoriedad\n",
    "import random\n",
    "\n",
    "# Omitir warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Leemos los datos limpios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lectura de los datos limpios (carpeta clean dentro de data)\n",
    "datos = pd.read_csv('../data/clean/clean_data.csv', engine='python')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### División de los datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Antes de dividir el dataset, lo barajamos, por si los datos pudiesen tener algún tipo de orden que perjudicase la aleatoriedad de los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(1234)\n",
    "datos = shuffle(datos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dividimos el dataset en tres tramos. El tramo train se utiliza para entrenar los modelos. El tramo de validación, se reserva para la optimización de hiperparámetros del modelo ganador. El tramo test, para evaluar los resultados de los modelos entrenados. Entrenamos con el 60% de los datos, y el otro 40% se divide entre validación y test, utilizando un 20% en cada parte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, validation, test = np.split(datos.sample(frac=1, random_state=1234), \n",
    "                                   [int(0.6*len(datos)),\n",
    "                                   int(0.8*len(datos))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es muy importatne poner el random state aquí, de lo contrario, los resultados no serán reproductibles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Efectuamos la separacion entre variables predictoras y variables dependientes para cada tramo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tramo de entrenamiento \n",
    "X_train = train.drop(['loan_status'], axis=1)\n",
    "y_train = train['loan_status']\n",
    "\n",
    "# Tramo de validacion \n",
    "X_validation = validation.drop(['loan_status'], axis=1)\n",
    "y_validation = validation['loan_status']\n",
    "\n",
    "# Tramo de test\n",
    "X_test = test.drop(['loan_status'], axis=1)\n",
    "y_test = test['loan_status']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Guardamos los datos en carpetas separadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Train en carpeta de train\n",
    "X_train.to_csv('../data/train/X_train.csv', index=False)\n",
    "y_train.to_csv('../data/train/y_train.csv', index=False, header=['loan_status'])\n",
    "\n",
    "# Validación en carpeta de validación\n",
    "X_validation.to_csv('../data/validation/X_validation.csv', index=False)\n",
    "y_validation.to_csv('../data/validation/y_validation.csv', index=False, header=['loan_status'])\n",
    "\n",
    "# Test en carpeta de test \n",
    "X_test.to_csv('../data/test/X_test.csv', index=False)\n",
    "y_test.to_csv('../data/test/y_test.csv', index=False, header=['loan_status'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelo base"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El modelo base o modelo es aquel que asigna la media u otro valor simple como prediccion del suceso. En nuestro caso, el modelo base imputará el valor más frecuente"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construimos el modelo base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Construcción\n",
    "modelo_base = DummyClassifier(strategy='most_frequent', # predice el valor más drecuente en el dataset \n",
    "                              random_state=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DummyClassifier(random_state=1234, strategy='most_frequent')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Entrenamiento \n",
    "modelo_base.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7494780341630697"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Accuracy del modelo \n",
    "modelo_base.score(X_test, y_test) # un 74.74%. Es coherente, es la proporción real "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Guardamos el modelo base para las comparaciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "nombre = '../models/trained_models/modelo_base.sav'\n",
    "pickle.dump(modelo_base, open(nombre, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como vamos a tener que guardar modelos en numerosas ocasiones, definimos una función"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def guardar_modelo(modelo, ubicacion):\n",
    "      '''\n",
    "    Función para guardar objetos de tipo modelo:\n",
    "    - modelo: el objeto con el modelo entrenado.\n",
    "    - ubicación: la carpeta de destino.\n",
    "    Los modelos se guardan en formato .sav, en la carpeta \n",
    "    trained_models, dentro de models.\n",
    "    \n",
    "    '''\n",
    "    pickle.dump(modelo, open(ubicacion, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construcción de los Pipelines "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los Pipeline resultan una herramienta crucial en el proceso de Machine Learning. Facilitan el proceso, por el hecho de que todos los pasos pueden agruparse de manera secuencial en otros pasos más genericos.   \n",
    "\n",
    "Además, al poder guardarlos como objetos, es posible volver a cargarlos, y así se optimiza el código, evitando el copiado y pegado que encarece el coste de solucionar errores. Lo más interesante de los Pipelines es que se pueden unir a los modelos, de tal manera que los datos pasan por el Pipeline y se procesan antes de entrar al modelo.  \n",
    "\n",
    "Lo mejor de todo, al guardar el modelo, el Pipeline también se guarda, haciendo que no haya problemas a la hora de predecir sobre datos nuevos, puesto que todo el proceso de codificación e imputación se ejecuta justo antes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el fondo, los Pipelines no son más que una serie de instrucciones, de pasos predefinidos que se ejecutan en un orden específico. Vamos a utilizar un transformador de columnas. Dicho transformador se aplicará tanto a columnas numéricas como a columnas categóricas, distinción que deberá quedar explicitada. Para cada una de estas transformaciones, habrá que tratar los valores perdidos, escalar los datos...etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformador de variables numericas: se encargará de imputar los valores perdidos y de escalar los datos (en variables \n",
    "# numéricas)\n",
    "\n",
    "transformador_numerico = Pipeline(steps=[\n",
    "    \n",
    "    # Paso 1: imputación de NA, nombre 'imputador'\n",
    "    ('imputador', SimpleImputer(strategy='median')), # utilizamos la mediana para imputar \n",
    "    \n",
    "    # Paso 2: escalar las variables, nombre 'escalador'\n",
    "    ('escalador', StandardScaler())])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El escalado de las variables numéricas se hace para mejorar el rendimiento de los modelos.  \n",
    "\n",
    "En cuanto a la imputación de valores perdidos, existen múltiples métodos. Uno de los más interesantes nos pareció el KNNImputer, que utiliza el algoritmo de clúster k-Nearest Neighbor para agrupar las observaciones en función de sus características, y así poder imputar el valor perdido.   \n",
    "\n",
    "Probamos este imputador, pero a causa del volumen de datos, el tiempo de cómputo se extendía demasiado, así que finalmente optamos por utilizar el imputador simple, con mediana, para evitar el efecto de los valores exrtremos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformador de variables categóricas: se encargará de imputar los valores perdidos de las variables categóricas y de \n",
    "# codificarlas, para que los modelos funcionen\n",
    "\n",
    "transformador_categorico = Pipeline(steps=[\n",
    "    \n",
    "    # Paso 1: imputación de NA, nombre 'imputador'\n",
    "    ('imputador', SimpleImputer(strategy='constant', fill_value='perdido')), # aquí hay que decirle qué queremos que ponga\n",
    "    # No utilizamos kNN porque la cantidad de NA's en las categóricas eternizaria el proceso\n",
    "    \n",
    "    \n",
    "    # Paso 2: codificación de variables categóricas, nombre 'onehot', usamos one hot encoding\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para el transformador categórcio, la imputación se realiza simplemente substituyendo el NA por \"perdido\".  \n",
    "\n",
    "Las variables categóricas tienen que ir codificadas, para lo que empleamos el famoso One Hot Encoding, que consiste en genrear columnas nuevas y utilizar 0's y 1's para determinar cuando una observación presenta dicha característica. Hay que tener cuidado con este método, porque de una columna categórica con 5 posibles valores, generará 5 nuevas columnas (la original se elimina). Esto puede ser un problema para variables con muchas categorías, ya que pordía ensanchar el dataset, algo no deseable en Machine Learning.  \n",
    "\n",
    "En cuanto a las categorías nuevas, el método seguido es simplemente ignorar dicha categoría"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Guardamos los nombres de las variables numéricas (formatos int64 y float) y las categóricas (object) para poder \n",
    "usarlos en los pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Las numéricas\n",
    "variables_numericas = datos.select_dtypes(include=['int64', 'float64']).columns\n",
    "\n",
    "# Las categóricas (excluida la dependiente)\n",
    "variables_categoricas = datos.select_dtypes(include=['object']).drop(['loan_status'], axis=1).columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construimos el preprocesador, con todas las piezas definidas antes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocesador = ColumnTransformer(\n",
    "    transformers=[\n",
    "        # Primer transformador, lo llamamos numericas: aplica los pasos del trasnformador numerico\n",
    "        # a las variables de 'variables_numericas'\n",
    "        ('numericas', transformador_numerico, variables_numericas),\n",
    "        \n",
    "        # Segundo trasnformador, lo llamamos categoricas: aplica los pasos del transformacor categórico\n",
    "        # a las variables de 'variables_categoricas'\n",
    "        ('categoricas', transformador_categorico, variables_categoricas)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como el Pipeline lo vamos a utilizar en distintos notebooks, lo guardamos con `pickle` para facilitar su uso, y así no  tener que repetir el código"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "nombre = '../pipelines/preprocesador.sav'\n",
    "pickle.dump(preprocesador, open(nombre, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regresión logística"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La regresión logística utiliza la función Sigmoide para devolver probabilidades de pertenencia a un grupo. Se ajusta por máxima verosimilitud, lo que quiere decir que en realidad se ajustan múltiples funciones Sigmoide, hasta que se  encuentre aquella que asigna a los puntos la probabilidad más cercana a su  grupo real. A continuación la implementamos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Montamos el clasificador\n",
    "regresion_logistica = Pipeline(steps=[\n",
    "    ('preprocesador', preprocesador), # primero se pre-procesan los datos\n",
    "    \n",
    "    ('clasificador', LogisticRegressionCV(cv=10, # validación cruzada de 10 folds\n",
    "                                          random_state=1234))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('preprocesador',\n",
       "                 ColumnTransformer(transformers=[('numericas',\n",
       "                                                  Pipeline(steps=[('imputador',\n",
       "                                                                   SimpleImputer(strategy='median')),\n",
       "                                                                  ('escalador',\n",
       "                                                                   StandardScaler())]),\n",
       "                                                  Index(['loan_amnt', 'int_rate', 'annual_inc', 'dti', 'annual_inc_joint',\n",
       "       'dti_joint', 'mort_acc'],\n",
       "      dtype='object')),\n",
       "                                                 ('categoricas',\n",
       "                                                  Pipeline(steps=[('imputador',\n",
       "                                                                   SimpleImputer(fill_value='perdido',\n",
       "                                                                                 strategy='constant')),\n",
       "                                                                  ('onehot',\n",
       "                                                                   OneHotEncoder(handle_unknown='ignore'))]),\n",
       "                                                  Index(['term', 'grade', 'emp_length', 'home_ownership', 'purpose', 'pub_rec',\n",
       "       'application_type', 'pub_rec_bankruptcies', 'rating_fico'],\n",
       "      dtype='object'))])),\n",
       "                ('clasificador',\n",
       "                 LogisticRegressionCV(cv=10, random_state=1234))])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Entrenamos el modelo\n",
    "regresion_logistica.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7874286909423974"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regresion_logistica.score(X_train, y_train) # 78.74% en entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.787362476876643"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regresion_logistica.score(X_test, y_test) # 78.73%, en test, muy similar, descartamos overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Guardamos el modelo de regresión logística"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "guardar_modelo(regresion_logistica, '../models/trained_models/logistic_regression.sav')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
